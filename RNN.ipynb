{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T02:11:19.702010Z",
     "start_time": "2024-10-04T02:11:12.225492Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'contractions'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcontractions\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'contractions'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "import contractions\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.python.keras.metrics import Recall, Precision\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a0f57a5c28878e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T02:11:32.048864Z",
     "start_time": "2024-10-04T02:11:32.036408Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    expanded_text = contractions.fix(text)\n",
    "    # split into words\n",
    "    tokens = word_tokenize(expanded_text)\n",
    "    # convert to lower case\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    # remove punctuation from each word\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b4b6172a452c4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T21:12:47.880921Z",
     "start_time": "2024-10-01T21:12:47.866908Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load dataset with UTF-8 encoding to avoid UnicodeDecodeError\n",
    "def load_reviews(directory):\n",
    "    reviews = []\n",
    "    labels = []\n",
    "    for label_type in ['pos', 'neg']:\n",
    "        label = 1 if label_type == 'pos' else 0  # 1 for positive, 0 for negative\n",
    "        dir_name = os.path.join(directory, label_type)\n",
    "        for fname in os.listdir(dir_name):\n",
    "            if fname.endswith('.txt'):\n",
    "                with open(os.path.join(dir_name, fname), 'r', encoding='utf-8') as file:  # Specify UTF-8 encoding\n",
    "                    reviews.append(preprocess_text(file.read()))  # Preprocess the text\n",
    "                    labels.append(label)\n",
    "    return reviews, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff6e85a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'contractions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load and preprocess training and testing data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_reviews, train_labels \u001b[38;5;241m=\u001b[39m \u001b[43mload_reviews\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m test_reviews, test_labels \u001b[38;5;241m=\u001b[39m load_reviews(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[18], line 11\u001b[0m, in \u001b[0;36mload_reviews\u001b[1;34m(directory)\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m fname\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     10\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dir_name, fname), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:  \u001b[38;5;66;03m# Specify UTF-8 encoding\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m                 reviews\u001b[38;5;241m.\u001b[39mappend(\u001b[43mpreprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# Preprocess the text\u001b[39;00m\n\u001b[0;32m     12\u001b[0m                 labels\u001b[38;5;241m.\u001b[39mappend(label)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reviews, labels\n",
      "Cell \u001b[1;32mIn[17], line 2\u001b[0m, in \u001b[0;36mpreprocess_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_text\u001b[39m(text):\n\u001b[1;32m----> 2\u001b[0m     expanded_text \u001b[38;5;241m=\u001b[39m \u001b[43mcontractions\u001b[49m\u001b[38;5;241m.\u001b[39mfix(text)\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# split into words\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m word_tokenize(expanded_text)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'contractions' is not defined"
     ]
    }
   ],
   "source": [
    "# Load and preprocess training and testing data\n",
    "train_reviews, train_labels = load_reviews('train/')\n",
    "test_reviews, test_labels = load_reviews('test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d0dba1e3314856",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T21:12:47.922079Z",
     "start_time": "2024-10-01T21:12:47.904625Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_reviews' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Split training data into train and validation sets\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m X_train, X_val, y_train, y_val \u001b[38;5;241m=\u001b[39m train_test_split(\u001b[43mtrain_reviews\u001b[49m, train_labels, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_reviews' is not defined"
     ]
    }
   ],
   "source": [
    "# Split training data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_reviews, train_labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "517151462a0acf33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T21:12:48.069969Z",
     "start_time": "2024-10-01T21:12:48.057701Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenization and padding\n",
    "tokenizer = Tokenizer(num_words=10000)  # Limit vocabulary to 10,000 most common words\n",
    "tokenizer.fit_on_texts(X_train)  # Fit tokenizer on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86977b045fc57fd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T21:25:17.256206Z",
     "start_time": "2024-10-01T21:12:48.157060Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load and preprocess training and testing data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_reviews, train_labels \u001b[38;5;241m=\u001b[39m \u001b[43mload_reviews\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m test_reviews, test_labels \u001b[38;5;241m=\u001b[39m load_reviews(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 10\u001b[0m, in \u001b[0;36mload_reviews\u001b[1;34m(directory)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fname \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(dir_name):\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m fname\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m---> 10\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdir_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:  \u001b[38;5;66;03m# Specify UTF-8 encoding\u001b[39;00m\n\u001b[0;32m     11\u001b[0m             reviews\u001b[38;5;241m.\u001b[39mappend(preprocess_text(file\u001b[38;5;241m.\u001b[39mread()))  \u001b[38;5;66;03m# Preprocess the text\u001b[39;00m\n\u001b[0;32m     12\u001b[0m             labels\u001b[38;5;241m.\u001b[39mappend(label)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Yujin Chen\\Documents\\GitHub\\ICS661Assignment-2\\.conda\\lib\\codecs.py:309\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.__init__\u001b[1;34m(self, errors)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBufferedIncrementalDecoder\u001b[39;00m(IncrementalDecoder):\n\u001b[0;32m    304\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;124;03m    This subclass of IncrementalDecoder can be used as the baseclass for an\u001b[39;00m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;124;03m    incremental decoder if the decoder must be able to handle incomplete\u001b[39;00m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;124;03m    byte sequences.\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 309\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    310\u001b[0m         IncrementalDecoder\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, errors)\n\u001b[0;32m    311\u001b[0m         \u001b[38;5;66;03m# undecoded input that is kept between calls to decode()\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Convert text to sequences of integers\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_val_seq = tokenizer.texts_to_sequences(X_val)\n",
    "X_test_seq = tokenizer.texts_to_sequences(test_reviews)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78d4cc2aba105c8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T21:25:17.369849Z",
     "start_time": "2024-10-01T21:25:17.324597Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pad sequences to ensure they all have the same length\n",
    "maxlen = 300  # You can adjust this depending on the average review length\n",
    "X_train_pad = pad_sequences(X_train_seq , maxlen=maxlen)\n",
    "X_val_pad = pad_sequences(X_val_seq , maxlen=maxlen)\n",
    "X_test_pad = pad_sequences(X_test_seq , maxlen=maxlen)\n",
    "\n",
    "# Define vocab_size and embedding_dim\n",
    "vocab_size = 10000  # Matches the tokenizer's num_words\n",
    "embedding_dim = 100 # You can adjust this based on your needs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c65ca421b15d09b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T21:25:21.932258Z",
     "start_time": "2024-10-01T21:25:17.710189Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create instances of Precision and Recall outside the metric function\n",
    "precision_metric = Precision(name='precision')\n",
    "recall_metric = Recall(name='recall')\n",
    "\n",
    "# Custom F1 score metric\n",
    "def f1_score(y_true, y_pred):\n",
    "    precision = precision_metric(y_true, y_pred)\n",
    "    recall = recall_metric(y_true, y_pred)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
    "    return f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45883e1fa5a5aa76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T21:25:29.638701Z",
     "start_time": "2024-10-01T21:25:21.964903Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert labels to NumPy arrays\n",
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "970eff65d690beb9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T21:25:30.832903Z",
     "start_time": "2024-10-01T21:25:29.690693Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, mask_zero=True, input_length=maxlen))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True)))  # First Bidirectional LSTM\n",
    "model.add(Bidirectional(LSTM(32)))   # Second Bidirectional LSTM\n",
    "model.add(Dense(32,kernel_regularizer=regularizers.l2(0.00001), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.0001),\n",
    "    metrics=['accuracy', precision_metric, recall_metric, f1_score]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "training_history = model.fit(\n",
    "    X_train_pad, y_train,\n",
    "    epochs= 10,\n",
    "    batch_size=512,\n",
    "    validation_data=(X_val_pad, y_val),\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6766fd574bf997f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T21:36:06.855306Z",
     "start_time": "2024-10-01T21:36:06.764811Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_labels = np.array(test_labels)\n",
    "score = model.evaluate(X_test_pad, test_labels, verbose=1)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(f'Test score or loss: {score[0]:.4f}')   # Loss\n",
    "print(f'Test accuracy: {score[1]:.4f}')         # Accuracy\n",
    "print(f'Test Precision: {score[2]:.4f}')        # Precision\n",
    "print(f'Test Recall: {score[3]:.4f}')           # Recall\n",
    "print(f'Test F1 Score: {score[4]:.4f}')         # F1 Score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "279d2c20374797d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T21:36:08.344013Z",
     "start_time": "2024-10-01T21:36:08.319143Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(training_history.history['accuracy'])\n",
    "plt.plot(training_history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(training_history.history['loss'], label='Train')\n",
    "ax.plot(training_history.history['val_loss'], label='Validation')\n",
    "ax.legend()\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Model loss')\n",
    "ax.set_ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "de29a7cddf4e6b7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T21:36:11.424206Z",
     "start_time": "2024-10-01T21:36:09.827620Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Arguments `target` and `output` must have the same rank (ndim). Received: target.shape=(None, 1, 1, 1, 1), output.shape=(None, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m training_history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train_pad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Adjust the number of epochs based on performance\u001b[39;49;00m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Tune batch size if necessary\u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val_pad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\nn.py:694\u001b[0m, in \u001b[0;36mbinary_crossentropy\u001b[1;34m(target, output, from_logits)\u001b[0m\n\u001b[0;32m    691\u001b[0m output \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(output)\n\u001b[0;32m    693\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(target\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape):\n\u001b[1;32m--> 694\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    695\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArguments `target` and `output` must have the same rank \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    696\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(ndim). Received: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    697\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, output.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    698\u001b[0m     )\n\u001b[0;32m    699\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e1, e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(target\u001b[38;5;241m.\u001b[39mshape, output\u001b[38;5;241m.\u001b[39mshape):\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e1 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e2 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e1 \u001b[38;5;241m!=\u001b[39m e2:\n",
      "\u001b[1;31mValueError\u001b[0m: Arguments `target` and `output` must have the same rank (ndim). Received: target.shape=(None, 1, 1, 1, 1), output.shape=(None, 1)"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "training_history = model.fit(\n",
    "    X_train_pad, y_train,\n",
    "    epochs=10,  # Adjust the number of epochs based on performance\n",
    "    batch_size=512,  # Tune batch size if necessary\n",
    "    validation_data=(X_val_pad, y_val),\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "857f4e4a01482e50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T21:25:33.745963100Z",
     "start_time": "2024-10-01T08:00:14.365292Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 53ms/step - accuracy: 0.8671 - loss: 0.4104 - precision_14: 0.8351 - recall_14: 0.8916\n",
      "Test score or loss: 0.4162\n",
      "Test accuracy: 0.8700\n",
      "Test Precision: 0.8369\n",
      "Test Recall: 0.8908\n",
      "Test F1 Score: 0.8630\n"
     ]
    }
   ],
   "source": [
    "# Plot F1 score\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(training_history.history['f1_score'])\n",
    "ax.set_title('Training F1 Score')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('F1 Score')\n",
    "ax.legend(['F1 Score'], loc='upper left')\n",
    "ax.set_ylim(0.5, 1)\n",
    "plt.show()\n",
    "\n",
    "# Plot precision and recall\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(training_history.history['precision'])  \n",
    "ax.plot(training_history.history['recall'])    \n",
    "ax.set_title('Training Precision and Recall')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Metrics')\n",
    "ax.set_ylim(0.7, 1) \n",
    "ax.legend(['Precision', 'Recall'], loc='upper left')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
